{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytreenet as ptn\n",
    "from copy import deepcopy\n",
    "from scipy.linalg import expm\n",
    "from pytreenet.util import copy_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_prime_dict(mps, mps_tilde):\n",
    "    \"\"\"\"\n",
    "    Function to build the A_prime matrixes in A.2\n",
    "    Args:\n",
    "        mps: MPS object\n",
    "        mps_tilde: MPS object\n",
    "    Returns:\n",
    "        tensors_prime: Dictionary with the A_prime matrices\n",
    "    \"\"\"\n",
    "    tensors = []\n",
    "    tensors_prime = {}\n",
    "\n",
    "    # Iterate over the sites\n",
    "    for i in range(len(mps.nodes) - 1):\n",
    "        # Extract the tensors\n",
    "        A1 = mps.tensors[f\"site{i}\"][..., 0]\n",
    "        A1_tilde = mps_tilde.tensors[f\"site{i}\"][..., 0]\n",
    "        A2 = mps.tensors[f\"site{i}\"][..., 1]\n",
    "        A2_tilde = mps_tilde.tensors[f\"site{i}\"][..., 1]\n",
    "\n",
    "        # Combine the tensors\n",
    "        tensors.append(A1 + A2)\n",
    "        tensors.append(A1_tilde + A2_tilde)\n",
    "\n",
    "        # Create the combined tensor\n",
    "        A_0 = np.block([[A1, np.zeros_like(A1)], [np.zeros_like(A1), A1_tilde]])\n",
    "        A_1 = np.block([[np.zeros_like(A2), A2], [A2_tilde, np.zeros_like(A2)]])\n",
    "        A_prime = A_0 + A_1\n",
    "        tensors_prime[mps.nodes[f\"site{i}\"].identifier] = A_prime\n",
    "\n",
    "    # Handle the last site\n",
    "    i = len(mps.nodes) - 1\n",
    "    C1 = mps.tensors[f\"site{i}\"][..., 0].reshape(mps.tensors[f\"site{i}\"][..., 0].shape[0], 1)\n",
    "    C1_tilde = mps_tilde.tensors[f\"site{i}\"][..., 0].reshape(mps_tilde.tensors[f\"site{i}\"][..., 0].shape[0], 1)\n",
    "    C2 = mps.tensors[f\"site{i}\"][..., 1].reshape(mps.tensors[f\"site{i}\"][..., 1].shape[0], 1)\n",
    "    C2_tilde = mps_tilde.tensors[f\"site{i}\"][..., 1].reshape(mps_tilde.tensors[f\"site{i}\"][..., 1].shape[0], 1)\n",
    "\n",
    "    tensors.append(C1 + C2)\n",
    "    tensors.append(C1_tilde + C2_tilde)\n",
    "\n",
    "    C_0 = np.block([[C1], [C1_tilde]])\n",
    "    C_1 = np.block([[C2], [C2_tilde]])\n",
    "    C_prime = C_0 + C_1\n",
    "    tensors_prime[mps.nodes[f\"site{i}\"].identifier] = C_prime\n",
    "\n",
    "    return tensors_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(739827.5689470959+5.820766091346741e-11j)\n",
      "(1+4.163336342344337e-17j)\n",
      "(1+4.163336342344337e-17j)\n",
      "contraction befor normalization :  (2655.6863640738857-5.684341886080802e-14j)\n",
      "(2655.6863640738857-5.684341886080802e-14j)\n",
      "(1.0000000000000002+0j)\n"
     ]
    }
   ],
   "source": [
    "##------------------------------------##\n",
    "## Test the functions : normalize_ttn ##\n",
    "##------------------------------------##\n",
    "\n",
    "## MPS ##\n",
    "shapes = [(5, 2), (5, 7, 2), (7, 3, 2), (3, 6, 2), (6, 30, 2), (30, 2)]\n",
    "tensors1 = [ptn.crandn(shape) for shape in shapes]\n",
    "mps1 = ptn.MatrixProductState.from_tensor_list(tensors1,root_site=5,node_prefix=\"site\")\n",
    "\n",
    "print( ptn.contract_two_ttns(mps1,mps1.conjugate()))\n",
    "mps1_normalized = mps1.normalize_ttn(to_copy = False) # default is False\n",
    "print( ptn.contract_two_ttns(mps1,mps1.conjugate()))\n",
    "print(ptn.contract_two_ttns(mps1_normalized,mps1_normalized.conjugate()))\n",
    "\n",
    "## TTN ##\n",
    "ttn1 = ptn.random_big_ttns_two_root_children()\n",
    "\n",
    "print(\"contraction befor normalization : \" , ptn.contract_two_ttns(ttn1,ttn1.conjugate()))\n",
    "ttn1_normalized = ttn1.normalize_ttn(to_copy = True)\n",
    "print( ptn.contract_two_ttns(ttn1,ttn1.conjugate()))\n",
    "print( ptn.contract_two_ttns(ttn1_normalized,ttn1_normalized.conjugate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "(0.9999999999999998+0j)\n",
      "(0.9999999999999989+4.490731800328466e-18j)\n",
      "(0.9999999999999958+9.583479375679694e-18j)\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "ttn1 = ptn.random_big_ttns_two_root_children()\n",
    "ttn2 = ptn.random_big_ttns_two_root_children()\n",
    "##---------------------##\n",
    "## Full density tensor ##\n",
    "##---------------------##\n",
    "\n",
    "## For pure states : (density tensor)^2 = density tensor \n",
    "pho , order = ttn1.density_tensor() \n",
    "# order = ['site0', 'site1', 'site2', 'site3', 'site4', 'site5', 'site6', 'site7']\n",
    "# density tensor * density tensor = density tensor :\n",
    "tensor = np.tensordot(pho,pho,axes=((8,9,10,11,12,13,14,15),(0,1,2,3,4,5,6,7)))\n",
    "print(np.allclose(tensor,pho))\n",
    "\n",
    "## trace of density tensor = 1\n",
    "def tensor_trace(pho):\n",
    "    \"\"\"\"\n",
    "    Args :\n",
    "        pho : Tensor\n",
    "    Returns :\n",
    "        trace : float\n",
    "    \"\"\"\n",
    "    for _ in range(pho.ndim//2):\n",
    "        pho = np.trace(pho, axis1 = 0, axis2 = pho.ndim//2)\n",
    "    return pho\n",
    "\n",
    "print(np.allclose(tensor_trace(pho),1))\n",
    "\n",
    "##---------------------------##\n",
    "## Full density tensor(ttno) ##\n",
    "##---------------------------##\n",
    "\n",
    "ttn = ptn.random_big_ttns_two_root_children()\n",
    "pho_ttno = ttn.density_ttno()\n",
    "\n",
    "## trace of density tensor = 1\n",
    "def ttno_trace(pho):\n",
    "    ttno_cct = deepcopy(pho)\n",
    "    ttno_cct = ttno_cct.completely_contract_tree()[0] \n",
    "    for _ in range(ttno_cct.ndim//2):\n",
    "        ttno_cct = np.trace(ttno_cct, axis1 = 0, axis2 = 1)\n",
    "    return ttno_cct\n",
    "\n",
    "print(np.allclose(ttno_trace(pho_ttno),1))\n",
    "\n",
    "## check density_ttno and density_tensor compatibility : \n",
    "ttn = ptn.random_small_ttns()\n",
    "pho , order = ttn.density_tensor() \n",
    "pho_ttno = ttn.density_ttno()\n",
    "np.allclose(pho_ttno.completely_contract_tree()[0],pho.transpose(0,3,1,4,2,5))\n",
    "\n",
    "##------------------------##\n",
    "## Reduced density tensor ##\n",
    "##------------------------##\n",
    "\n",
    "ttn = ptn.random_big_ttns_two_root_children()\n",
    "node_id = 'site0'\n",
    "# first method :\n",
    "pho_ref_1 = ttn.reduced_density_matrix_dict()[node_id]\n",
    "pho_ref_2 = ttn.reduced_density_matrix(node_id)\n",
    "# second method :\n",
    "pho = ttn.reduced_density_matrix_2(node_id)\n",
    "\n",
    "# trace of reduced density tensor = 1\n",
    "print(np.trace(pho, axis1 = 0, axis2 = 1))\n",
    "print(np.trace(pho_ref_1, axis1 = 0, axis2 = 1))\n",
    "print(np.trace(pho_ref_2, axis1 = 0, axis2 = 1))\n",
    "\n",
    "# Check Hermiticity : \n",
    "print(np.allclose(pho,pho.conjugate().T))\n",
    "print(np.allclose(pho_ref_1,pho_ref_1.conjugate().T))\n",
    "print(np.allclose(pho_ref_2,pho_ref_2.conjugate().T))\n",
    "\n",
    "# calculate explicitly the reduced density tensor of a node in a tree tensor network\n",
    "node_id = 'site4'\n",
    "ttn = ttn.normalize_ttn(to_copy=True)\n",
    "ttn.canonical_form(node_id , ptn.SVDParameters())\n",
    "C = ttn.tensors[node_id]\n",
    "reduced_density_matrix_dir = ptn.compute_transfer_tensor(C, tuple(range(C.ndim - 1)))\n",
    "\n",
    "\n",
    "reduced_density_matrix_ref_1 = ttn.reduced_density_matrix_dict()[node_id]\n",
    "reduced_density_matrix_ref_2 = ttn.reduced_density_matrix(node_id)\n",
    "\n",
    "print(np.allclose(reduced_density_matrix_dir,reduced_density_matrix_ref_1))\n",
    "print(np.allclose(reduced_density_matrix_dir,reduced_density_matrix_ref_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "First I did it with reduced_density_matrix_2, but I guess completely_contract_tree is more \n",
    "costly that moving the canoical center. I wonder why the results are\n",
    "not same!\n",
    "\"\"\"\n",
    "ttn = ptn.random_big_ttns_two_root_children()\n",
    "node_id = 'site2'\n",
    "pho = ttn.reduced_density_matrix_2(node_id)\n",
    "pho_tilde = ttn.reduced_density_matrix_dict()[node_id]\n",
    "print(np.allclose(pho,pho_tilde)) ######### Fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "##---------------------##\n",
    "## canonical_form :svd ##\n",
    "##---------------------##\n",
    "\n",
    "### MPS ###\n",
    "shapes = [(5, 2), (5, 7, 2), (7, 3, 2), (3, 6, 2), (6, 30, 2), (30, 2)]\n",
    "tensors1 = [ptn.crandn(shape) for shape in shapes]\n",
    "mps1 = ptn.MatrixProductState.from_tensor_list(tensors1,root_site=5,node_prefix=\"site\")\n",
    "\n",
    "## test canonical_form for \"site2\" :\n",
    "ref_contracted = ptn.contract_two_ttns(mps1,mps1.conjugate())\n",
    "mps1.canonical_form(\"site2\", ptn.SVDParameters())\n",
    "direct_contracted = ptn.compute_transfer_tensor(mps1.tensors[\"site2\"], tuple(range(mps1.tensors[\"site2\"].ndim )))\n",
    "print(np.allclose(ref_contracted, direct_contracted))\n",
    "\n",
    "### TTN ###\n",
    "ttn1 = ptn.random_big_ttns_two_root_children()\n",
    "\n",
    "# test canonical_form for \"site3\"\n",
    "direct_contracted = ptn.contract_two_ttns(ttn1,ttn1.conjugate())\n",
    "ttn1.canonical_form(\"site3\", ptn.SVDParameters())\n",
    "ref_contracted = ptn.compute_transfer_tensor(ttn1.tensors[\"site3\"], tuple(range(ttn1.tensors[\"site3\"].ndim )))\n",
    "print(np.allclose(ref_contracted, direct_contracted))\n",
    "\n",
    "\n",
    "# Check canonical_form with QR and SVD compatibility \n",
    "ttn = ptn.random_big_ttns_two_root_children()\n",
    "ttn1 = deepcopy(ttn)\n",
    "ttn1.canonical_form('site0', ptn.SVDParameters(max_bond_dim = np.inf, rel_tol= -np.inf, total_tol=-np.inf))\n",
    "ttn2 = deepcopy(ttn)\n",
    "ttn2.canonical_form('site0')\n",
    "a = ptn.compute_transfer_tensor(ttn1.tensors['site0'], (0,1,2))\n",
    "b = ptn.compute_transfer_tensor(ttn2.tensors['site0'], (0,1,2))\n",
    "c = ptn.contract_two_ttns(ttn,ttn.conjugate())\n",
    "print(np.allclose(a,b))\n",
    "print(np.allclose(a,c))\n",
    "print(np.allclose(b,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "(0.9999999999999998+0j)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "##-------------------------##\n",
    "## complete_canonical_form ##\n",
    "##-------------------------##\n",
    "\n",
    "# I defined new function because the canonical_form transforms ttns only to site cannonical form.\n",
    "#    - complete_canonical_form transforms all tensors to left Isometry \n",
    "#      with respect to root site controlled by SVD truncation parameters.\n",
    "\n",
    "### MPS ###\n",
    "shapes = [(5, 2), (5, 7, 2), (7, 3, 2), (3, 6, 2), (6, 30, 2), (30, 2)]\n",
    "tensors1 = [ptn.crandn(shape) for shape in shapes]\n",
    "mps1 = ptn.MatrixProductState.from_tensor_list(tensors1,root_site=5,node_prefix=\"site\")\n",
    "\n",
    "\n",
    "## test complete_canonical_form : \n",
    "mps1_norm = ptn.contract_two_ttns(mps1,mps1.conjugate())\n",
    "_ , norm = mps1.complete_canonical_form(ptn.SVDParameters())\n",
    "    # check the norm \n",
    "print(np.allclose(mps1_norm,norm))\n",
    "print(np.allclose(ptn.contract_two_ttns(mps1,mps1.conjugate()),1))\n",
    "    # check the orthogonality of root site = \"site5\"\n",
    "Identity = ptn.compute_transfer_tensor(mps1.tensors[mps1.root_id], (0,1) )\n",
    "print(np.allclose(Identity,1))\n",
    "    # check the orthogonality of \"site3\"\n",
    "Identity = ptn.compute_transfer_tensor(mps1.tensors[\"site3\"], (1,2) )\n",
    "print(np.allclose(Identity,np.eye(6)))\n",
    "    # check the orthogonality of leaf site = \"site0\"\n",
    "print(np.allclose(ptn.compute_transfer_tensor(mps1.tensors[\"site0\"], (1,) ),np.eye(2)))\n",
    "\n",
    "\n",
    "### TTN ###\n",
    "ttn1 = ptn.random_big_ttns_two_root_children()\n",
    "\n",
    "\n",
    "\n",
    "## test complete_canonical_form :\n",
    "ttn1_norm = ptn.contract_two_ttns(ttn1,ttn1.conjugate())\n",
    "_ , norm = ttn1.complete_canonical_form(ptn.SVDParameters())\n",
    "    # check the norm \n",
    "print(np.allclose(ttn1_norm,norm))\n",
    "print(np.allclose(ptn.contract_two_ttns(ttn1,ttn1.conjugate()),1)) \n",
    "    # check the orthogonality of root site = \"site0\"\n",
    "print(np.allclose(ptn.compute_transfer_tensor(ttn1.tensors[ttn1.root_id], (0,1,2) ),1))\n",
    "    # check the orthogonality of leaf site = \"site4\"\n",
    "print(np.allclose(ptn.compute_transfer_tensor(ttn1.tensors[\"site4\"], (1,) ),np.eye(2)))\n",
    "    # check the orthogonality of \"site3\"\n",
    "Identity = ptn.compute_transfer_tensor(ttn1.tensors[\"site3\"], (1,2,3) )\n",
    "print(np.allclose(Identity,np.eye(2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'site1': array([[ 0.25397585-0.37323946j, -0.26691782-0.23933904j],\n",
      "       [ 0.68601921-1.30767138j, -0.60245601+1.12681351j]]), 'site2': array([[-2.02941614-0.30079625j, -0.66850564-0.5442636j ],\n",
      "       [-0.18835916+0.67173573j,  0.2762695 +0.50995177j]]), 'site4': array([[ 0.08226436-0.87566713j, -0.21475914-0.75201694j],\n",
      "       [-0.16243023-1.24283638j, -0.30813163+0.10954597j]]), 'site3': array([[ 0.90729137+0.25735946j,  0.45024987+0.12249447j],\n",
      "       [-0.97081991+1.08448584j, -0.57950414+0.24301743j]]), 'site5': array([[0.18882298-0.75678626j, 0.0873076 +0.42551639j],\n",
      "       [1.36459378+0.43842693j, 0.21132063+0.50319474j]]), 'site0': array([[ 0.4361642 -1.33856301j, -0.31341537+0.72287677j],\n",
      "       [ 0.15550583+1.09071626j,  0.24883691-0.34280232j]])}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tp)\n\u001b[0;32m      3\u001b[0m H \u001b[38;5;241m=\u001b[39m ptn\u001b[38;5;241m.\u001b[39mHamiltonian(tp)\n\u001b[1;32m----> 4\u001b[0m hamiltonian \u001b[38;5;241m=\u001b[39m \u001b[43mptn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTTNO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hamiltonian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmps1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\ttno\\ttno.py:58\u001b[0m, in \u001b[0;36mTTNO.from_hamiltonian\u001b[1;34m(cls, hamiltonian, reference_tree)\u001b[0m\n\u001b[0;32m     56\u001b[0m ttno \u001b[38;5;241m=\u001b[39m TTNO()\n\u001b[0;32m     57\u001b[0m root_id \u001b[38;5;241m=\u001b[39m reference_tree\u001b[38;5;241m.\u001b[39mroot_id\n\u001b[1;32m---> 58\u001b[0m root_shape \u001b[38;5;241m=\u001b[39m \u001b[43mstate_diagram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_tensor_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mhamiltonian\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconversion_dictionary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m root_tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(root_shape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcomplex\u001b[39m)\n\u001b[0;32m     61\u001b[0m root_node \u001b[38;5;241m=\u001b[39m Node(identifier\u001b[38;5;241m=\u001b[39mroot_id)\n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\ttno\\state_diagram.py:314\u001b[0m, in \u001b[0;36mStateDiagram.obtain_tensor_shape\u001b[1;34m(self, node_id, conversion_dict)\u001b[0m\n\u001b[0;32m    312\u001b[0m he \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperedge_colls[node_id]\u001b[38;5;241m.\u001b[39mcontained_hyperedges[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    313\u001b[0m operator_label \u001b[38;5;241m=\u001b[39m he\u001b[38;5;241m.\u001b[39mlabel\n\u001b[1;32m--> 314\u001b[0m operator \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43moperator_label\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# Should be square operators\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m operator\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m operator\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "tp = ptn.random_tensor_product(mps1, num_operators= len(mps1))\n",
    "print(tp)\n",
    "H = ptn.Hamiltonian(tp)\n",
    "hamiltonian = ptn.TTNO.from_hamiltonian(H,mps1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krylov space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NoConnectionException",
     "evalue": "link_site0_with_site1 is not a neighbour of site0!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoConnectionException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m## start ##\u001b[39;00m\n\u001b[0;32m     31\u001b[0m Krylov_space \u001b[38;5;241m=\u001b[39m ptn\u001b[38;5;241m.\u001b[39mKrylov(mps1, ham_mps)\n\u001b[1;32m---> 32\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mKrylov_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(results[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m Krylov_space\u001b[38;5;241m.\u001b[39minitial_state)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\time_evolution\\Krylov.py:105\u001b[0m, in \u001b[0;36mKrylov.run\u001b[1;34m(self, num_steps)\u001b[0m\n\u001b[0;32m    103\u001b[0m results[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_state\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_steps\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m--> 105\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_1sproj_H\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m         results[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;66;03m# orthogonalize self.results[i+1] against self.results[i+1]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\time_evolution\\Krylov.py:148\u001b[0m, in \u001b[0;36mKrylov.apply_1sproj_H\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mmove_orthogonalization_center(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_path[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_partial_tree_cache()\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_first_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normal_update(node_id,i)  \n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\time_evolution\\Krylov.py:127\u001b[0m, in \u001b[0;36mKrylov._first_update\u001b[1;34m(self, node_id)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_orth_center(node_id)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_leaf_node(node_id)\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_site_and_link_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\time_evolution\\Krylov.py:122\u001b[0m, in \u001b[0;36mKrylov._update_site_and_link_2\u001b[1;34m(self, node_id, update_index)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m update_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morthogonalization_path)\n\u001b[0;32m    121\u001b[0m next_node_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morthogonalization_path[update_index][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_link_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_node_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\time_evolution\\Krylov.py:78\u001b[0m, in \u001b[0;36mKrylov._update_link_tensor\u001b[1;34m(self, node_id, next_node_id)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_link_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     75\u001b[0m                             next_node_id: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_updated_site(node_id, next_node_id)\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_ham_site\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     link_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_link_id(node_id, next_node_id)\n\u001b[0;32m     80\u001b[0m     link_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtensors[link_id]\n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\time_evolution\\Krylov.py:58\u001b[0m, in \u001b[0;36mKrylov._apply_ham_site\u001b[1;34m(self, node_id)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_ham_site\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_id: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 58\u001b[0m     hamiltonian_eff_site \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_effective_site_hamiltonian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     psi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtensors[node_id]\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtensors[node_id] \u001b[38;5;241m=\u001b[39m apply_hamiltonian(psi,hamiltonian_eff_site)\n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\time_evolution\\Krylov.py:29\u001b[0m, in \u001b[0;36mKrylov._get_effective_site_hamiltonian\u001b[1;34m(self, node_id)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_effective_site_hamiltonian\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_effective_site_hamiltonian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\time_evolution\\tdvp_algorithms\\tdvp_algorithm.py:264\u001b[0m, in \u001b[0;36mTDVPAlgorithm._get_effective_site_hamiltonian\u001b[1;34m(self, node_id)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_effective_site_hamiltonian\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    253\u001b[0m                                     node_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    254\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m    Obtains the effective site Hamiltonian as defined in Ref. [1]\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m     Eq. (16a) as a matrix.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m        np.ndarray: The effective site Hamiltonian\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_contract_all_except_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor_matricisation_half(tensor)\n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\time_evolution\\tdvp_algorithms\\tdvp_algorithm.py:248\u001b[0m, in \u001b[0;36mTDVPAlgorithm._contract_all_except_node\u001b[1;34m(self, target_node_id)\u001b[0m\n\u001b[0;32m    245\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtensordot(tensor, cached_tensor,\n\u001b[0;32m    246\u001b[0m                           axes\u001b[38;5;241m=\u001b[39m((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# Transposing to have correct leg order\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_tensor_leg_permutation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_node_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(tensor, axes\u001b[38;5;241m=\u001b[39maxes)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\time_evolution\\tdvp_algorithms\\tdvp_algorithm.py:195\u001b[0m, in \u001b[0;36mTDVPAlgorithm._find_tensor_leg_permutation\u001b[1;34m(self, node_id)\u001b[0m\n\u001b[0;32m    193\u001b[0m permutation \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m neighbour_id \u001b[38;5;129;01min\u001b[39;00m state_node\u001b[38;5;241m.\u001b[39mneighbouring_nodes():\n\u001b[1;32m--> 195\u001b[0m     hamiltonian_index \u001b[38;5;241m=\u001b[39m \u001b[43mhamiltonian_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbour_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     permutation\u001b[38;5;241m.\u001b[39mappend(hamiltonian_index)\n\u001b[0;32m    197\u001b[0m output_legs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\edpou\\Desktop\\Ed\\PyTreeNet-Project_01\\pytreenet\\core\\graph_node.py:218\u001b[0m, in \u001b[0;36mGraphNode.neighbour_index\u001b[1;34m(self, node_id)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mindex(node_id) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnparents()\n\u001b[0;32m    217\u001b[0m errstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a neighbour of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 218\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NoConnectionException(errstr)\n",
      "\u001b[1;31mNoConnectionException\u001b[0m: link_site0_with_site1 is not a neighbour of site0!"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In first method, I applied one site projector at ecah step(along the same path as tdvp)\n",
    "\"\"\"\n",
    "# Initialize a random mps and ttn and their random hamiltonian : \n",
    "\n",
    "## MPS ##\n",
    "\"\"\"\n",
    "0 --- 1 --- 2 --- 3 --- 4 ---- 5\n",
    "|     |     |     |     |      |\n",
    "\n",
    "# order of legs =  [right_child, left_child, open_leg]\n",
    "\"\"\"\n",
    "shapes = [(5, 2), (5, 7, 2), (7, 3, 2), (3, 6, 2), (6, 30, 2), (30, 2)]\n",
    "tensors1 = [ptn.crandn(shape) for shape in shapes]\n",
    "mps1 = ptn.MatrixProductState.from_tensor_list(tensors1,root_site=5,node_prefix=\"site\")\n",
    "\n",
    "\"\"\"\n",
    "Question : \n",
    "tp = ptn.random_tensor_product(mps1, num_operators= len(mps1))\n",
    "H = ptn.Hamiltonian(tp)\n",
    "hamiltonian = ptn.TTNO.from_hamiltonian(H,mps1)\n",
    "this method no longer works (Error: state_diagram.obtain_tensor_shape)\n",
    "\"\"\"\n",
    "tensor = ptn.crandn([2,2,2,2,2,2,\n",
    "                     2,2,2,2,2,2])\n",
    "leg_dict = {\"site0\": 0, \"site1\": 1, \"site2\": 2, \"site3\": 3, \"site4\": 4, \"site5\": 5}\n",
    "ham_mps = ptn.TTNO.from_tensor(mps1, tensor, leg_dict)\n",
    "\n",
    "\n",
    "## start ##\n",
    "Krylov_space = ptn.Krylov(mps1, ham_mps)\n",
    "results = Krylov_space.run(num_steps=4)\n",
    "print(results[0] == Krylov_space.initial_state)\n",
    "print(len(results) == 5)\n",
    "print(results[0] == Krylov_space.initial_state)\n",
    "# Krylov_space.results[1] = P*hamiltonian |inisial_state>\n",
    "# Krylov_space.results[2] = P*hamiltonian^2 |Krylov_space.results[1]>\n",
    "# Krylov_space.results[3] = P*hamiltonian^3 |Krylov_space.results[2]>\n",
    "# Krylov_space.results[4] = P*hamiltonian^4 |Krylov_space.results[3\n",
    "\n",
    "\n",
    "## TTN ##\n",
    "ttn1 = ptn.random_big_ttns_two_root_children()\n",
    "ham_ttn = ptn.TTNO.from_hamiltonian(ptn.random_hamiltonian_compatible(),ttn1)\n",
    "\n",
    "Krylov_space = ptn.Krylov(ttn1, ham_ttn, 1,10,  ptn.TensorProduct({\"site0\": ptn.pauli_matrices()[0]}))\n",
    "results = Krylov_space.run(4 , ptn.SVDParameters())\n",
    "print(results[0] == Krylov_space.initial_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ortogonalize ttn against ttn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I wanted to orthogonalize the outcome at each step to the previous states, but \n",
    "I could not find a way to ortogonilize two ttns against each other.\n",
    "I tried to orthogonalize each tensors seperately, but it did not work.\n",
    "\"\"\"\n",
    "\n",
    "def orthogonalize_against(ttn1 , ttn2):\n",
    "    \"\"\"\n",
    "    Orthogonalize all locale tensors of ttn1 against ttn2.\n",
    "    Args:\n",
    "        ttn1 : TreeTensorNetwork\n",
    "        ttn2 : TreeTensorNetwork\n",
    "    \"\"\"\n",
    "    for i in range(len(ttn1.nodes)):\n",
    "        shape = ttn1.tensors[f\"site{i}\"].shape\n",
    "        a = ttn1.tensors[f\"site{i}\"].reshape(-1)\n",
    "        b = deepcopy(ttn2.tensors[f\"site{i}\"])\n",
    "        b = normalize_tensor(b)\n",
    "        b = b.reshape(-1)\n",
    "        prod = np.tensordot(b.conj(), a , axes = (0,0) )\n",
    "        a = a - b * prod\n",
    "        a = a / np.sqrt(ptn.contract_two_ttns(ttn1,ttn1.conjugate()))\n",
    "        ttn1.tensors[f\"site{i}\"] = np.reshape(a,shape)\n",
    "    return ttn1 , ttn2\n",
    "\n",
    "def _tensor(tensor): \n",
    "    \"\"\"\n",
    "      a tensor.\n",
    "     Args:\n",
    "          tensor : np.ndarray\n",
    "     Returns : \n",
    "          The normalized tensor.\n",
    "     \"\"\"\n",
    "    indices  = range(tensor.ndim)\n",
    "    norm = np.sqrt(np.tensordot(tensor,tensor.conj(), axes = (indices , indices) ))\n",
    "    return tensor / norm\n",
    "  \n",
    "## Test : (fails)\n",
    "shapes = [(5, 2), (5, 7, 2), (7, 3, 2), (3, 6, 2), (6, 30, 2), (30, 2)]\n",
    "tensors1 = [ptn.crandn(shape) for shape in shapes]\n",
    "tensors2 = [ptn.crandn(shape) for shape in shapes]\n",
    "mps1 = ptn.MatrixProductState.from_tensor_list(tensors1,root_site=5,node_prefix=\"site\")\n",
    "mps2 = ptn.MatrixProductState.from_tensor_list(tensors2,root_site=5,node_prefix=\"site\")\n",
    "orthogonalize_against(mps1,mps2)\n",
    "\n",
    "a = mps1.tensors[\"site2\"]\n",
    "b = mps2.tensors[\"site2\"]\n",
    "print(np.allclose(np.tensordot(b.conj(), a , axes = ((0,1,2),(0,1,2))), 0))\n",
    "print(np.allclose(ptn.contract_two_ttns(mps1,mps2.conjugate()),0)) # not orthogonal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krylov_second_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn this method, I contracted hamiltonian(TTNO) to the state(TTN) at each site. \\nthe bond dimension was growing exponentially, so at each step I transformed\\nthe state to the complete canonical form, where bond dimension are controlled by\\nSVD truncation parameters.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this method, I contracted hamiltonian(TTNO) to the state(TTN) at each site. \n",
    "the bond dimension was growing exponentially, so at each step I transformed\n",
    "the state to the complete canonical form, where bond dimension are controlled by\n",
    "SVD truncation parameters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# initialize mps : mps1\n",
    "\n",
    "# Antiferromagnet state\n",
    "mps_up = ptn.MatrixProductState.constant_product_state(state_value = 0,\n",
    "                                                     dimension =  2,\n",
    "                                                     num_sites = 15,\n",
    "                                                     node_prefix = \"site\", # default \n",
    "                                                     root_site = 14)\n",
    "\n",
    "mps_down = ptn.MatrixProductState.constant_product_state(state_value = 1,\n",
    "                                                     dimension =  2,\n",
    "                                                     num_sites = 15,\n",
    "                                                     node_prefix = \"site\", # default \n",
    "                                                     root_site = 14)    \n",
    "mps1 = deepcopy(mps_up)\n",
    "for i in range(len(mps1.nodes)):\n",
    "    if i % 2 == 0:\n",
    "       mps1.tensors[f\"site{i}\"] = mps_up.tensors[f\"site{i}\"]\n",
    "    else:\n",
    "       mps1.tensors[f\"site{i}\"] = mps_down.tensors[f\"site{i}\"]  \n",
    "\n",
    "\n",
    "# Initialize a random hamiltonian(TTNO) : mpo1\n",
    "\n",
    "X , Y , Z = ptn.pauli_matrices()\n",
    "possible_operators = [X, X@Y, Z@X, Z, Y, X@Y , Z@Z, Y@X, Z@Y ] \n",
    "sites = [f\"site{i}\" for i in range(len(mps1.nodes))]\n",
    "# List = ptn.random_symbolic_terms(num_of_terms=4, \n",
    "#                                 possible_operators=possible_operators, \n",
    "#                                 sites=sites, \n",
    "#                                 min_num_sites=2, \n",
    "#                                 max_num_sites=4, \n",
    "#                                 seed=None) ----> Error \n",
    "###########################################################\n",
    "List = ptn.random_terms(num_of_terms = 5,\n",
    "                        possible_operators = possible_operators,\n",
    "                        sites = sites,\n",
    "                        min_strength = 1,\n",
    "                        max_strength = 4,\n",
    "                        min_num_sites= 10,\n",
    "                        max_num_sites= 14)\n",
    "tp_list = [ptn.TensorProduct(term) for term in List]\n",
    "\n",
    "num_terms = 20\n",
    "tp_list = [ptn.random_tensor_product(mps1,10,[\"A\",\"B\",\"C\",\"D\"]) for _ in range(num_terms)]\n",
    "conversion_dictionary = {\"A\": ptn.random_hermitian_matrix(2),\n",
    "                         \"B\": ptn.random_hermitian_matrix(2),\n",
    "                         \"C\": ptn.random_hermitian_matrix(2),\n",
    "                         \"D\": ptn.random_hermitian_matrix(2),\n",
    "                         \"I2\": np.eye(2)}\n",
    "\n",
    "H = ptn.Hamiltonian(tp_list, conversion_dictionary=conversion_dictionary)\n",
    "H = H.pad_with_identities(mps1)\n",
    "ttno1 = ptn.TTNO.from_hamiltonian(H, mps1)\n",
    "##############################################################\n",
    "tp1 = ptn.random_tensor_product(mps1, num_operators= len(mps1), possible_operators = possible_operators)\n",
    "tp2 = ptn.random_tensor_product(mps1, num_operators= len(mps1), possible_operators = possible_operators)\n",
    "\n",
    "H = ptn.Hamiltonian([tp1,tp2])\n",
    "H = H.pad_with_identities(mps1)\n",
    "# mpo1 = ptn.TTNO.from_hamiltonian(H,mps1) ----> Error: StateDiagram.from_hamiltonian\n",
    "######################################################################################################\n",
    "\n",
    "conversion_dictionary = {}\n",
    "for i in range(len(mps1.nodes)):\n",
    "    conversion_dictionary[f\"{i}\"] = ptn.random_hermitian_matrix(2)\n",
    "for i in range(len(mps1.nodes)):\n",
    "    conversion_dictionary[f\"I{i}\"] = np.eye(i)\n",
    "\n",
    "dict = {}\n",
    "for i in range(len(mps1.nodes)):\n",
    "    dict[f\"site{i}\"] = f\"{i}\"\n",
    "tp1 = ptn.TensorProduct(dict)\n",
    "\n",
    "for i in range(len(mps1.nodes)):\n",
    "    dict[f\"site{i}\"] = f\"{len(mps1.nodes) - i-1}\"\n",
    "tp2 = ptn.TensorProduct(dict)\n",
    "\n",
    "H = ptn.Hamiltonian([tp1,tp2],conversion_dictionary)\n",
    "H = H.pad_with_identities(mps1)\n",
    "mpo1 = ptn.TTNO.from_hamiltonian(H,mps1) # All bond dimnsions are 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 17.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(0.9999999999999991-6.938893903907228e-18j)\n",
      "(0.9999999999999991+0j)\n",
      "(1.0000000000000013+1.3877787807814457e-17j)\n",
      "True\n",
      "(1, 1, 2)\n",
      "(3, 3, 2)\n",
      "(8, 8, 2)\n",
      "(21, 21, 2)\n",
      "(49, 46, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Krylov_space = ptn.Krylov_second_method(mps1, mpo1)\n",
    "Krylov_space.run(10, ptn.SVDParameters())\n",
    "results = Krylov_space.results\n",
    "\n",
    "\n",
    "# results[0] = initial_state\n",
    "print(results[0] == mps1)\n",
    "# results[1] = hamiltonian |inisial_state>\n",
    "# results[2] = hamiltonian^2 |Krylov_space.results[1]>\n",
    "# results[3] = hamiltonian^3 |Krylov_space.results[2]>\n",
    "# results[4] = hamiltonian^4 |Krylov_space.results[3]>\n",
    "# Check the norms \n",
    "print(ptn.contract_two_ttns(results[1],results[1].conjugate()))\n",
    "print(ptn.contract_two_ttns(results[5],results[5].conjugate()))\n",
    "print(ptn.contract_two_ttns(results[10],results[10].conjugate()))\n",
    "# check the orthogonality of root site = \"site14\"\n",
    "print(np.allclose(ptn.compute_transfer_tensor( results[2].tensors[ttn1.root_id], (0,)) , np.eye(2) ))\n",
    "# the growth of bind dimensions at \"site10\"\n",
    "print(results[0].tensors[\"site10\"].shape)\n",
    "print(results[2].tensors[\"site10\"].shape)\n",
    "print(results[4].tensors[\"site10\"].shape)\n",
    "print(results[6].tensors[\"site10\"].shape)\n",
    "print(results[8].tensors[\"site10\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps = results[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps.tensors[\"site13\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps, _ = mps.complete_canonical_form(ptn.SVDParameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Bond expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = [(4,2)] + [(4,4,2)]*13 + [(4,2)]\n",
    "tensors = [ptn.crandn(shape) for shape in shapes]\n",
    "mps = ptn.MatrixProductState.from_tensor_list(tensors,root_site=14,node_prefix=\"site\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10/21 [00:00<00:00, 44.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 50.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the states for bond expansion \n",
    "Krylov_space = ptn.Krylov(mps, mpo1, 1, 10, ptn.TensorProduct({\"site0\": ptn.pauli_matrices()[0]}))\n",
    "results = Krylov_space.run(20)\n",
    "\n",
    "mps1 = results[0]\n",
    "mps2 = results[1]\n",
    "mps3 = results[2]\n",
    "mps4 = results[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[3].tensors[\"site14\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "truncated_tensor_svd() got an unexpected keyword argument 'max_bond_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m U , S , B \u001b[38;5;241m=\u001b[39m \u001b[43mptn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncated_tensor_svd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmps1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msite14\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mmax_bond_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_tol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_tol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(U\u001b[38;5;241m.\u001b[39mshape , S\u001b[38;5;241m.\u001b[39mshape , B\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m B_dagger \u001b[38;5;241m=\u001b[39m B\u001b[38;5;241m.\u001b[39mconj()\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mTypeError\u001b[0m: truncated_tensor_svd() got an unexpected keyword argument 'max_bond_dim'"
     ]
    }
   ],
   "source": [
    "U , S , B = ptn.truncated_tensor_svd(mps1.tensors[\"site14\"], (1,), (0,),\n",
    "                                     max_bond_dim=np.inf , rel_tol=-np.inf, total_tol=-np.inf)\n",
    "print(U.shape , S.shape , B.shape)\n",
    "\n",
    "B_dagger = B.conj().T\n",
    "B_dagger_B = B_dagger @ B \n",
    "P = np.eye(B_dagger_B.shape[0]) - B_dagger_B\n",
    "\n",
    "pho_2 = reduced_density_matrix(mps2,\"site14\")\n",
    "pho_3 = reduced_density_matrix(mps3,\"site14\")\n",
    "pho_4 = reduced_density_matrix(mps4,\"site14\")\n",
    "pho_5 = reduced_density_matrix(mps5,\"site14\")\n",
    "pho_6 = reduced_density_matrix(mps6,\"site14\")\n",
    "pho_tilde = pho_2 + pho_3 + pho_4 + pho_5 + pho_6\n",
    "\n",
    "# pho_baar = P @ pho @ P ---> dimension mismatch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
